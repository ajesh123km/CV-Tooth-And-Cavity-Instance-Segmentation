{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tooth and Cavity Instance Segmentation\n",
    "Deep learning in dentistry holds a multitude of applications for identifying various dental conditions and diseases. Here, we train a deep-learning model for identifying and segmenting instances of the following:\n",
    "- Caries\n",
    "- Cavities\n",
    "- Teeth\n",
    "- Cracks\n",
    "\n",
    "**Instance Segmentation** requires a model to not only create an image mask to classify objects pixel-by-pixel, but also to create boundaries to separate multiple instances of the same object. While multiple architectures are available for performing instance segmentation, we focus on training and comparing the following models:\n",
    "- Mask RCNN\n",
    "- YOLOv8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Let's begin by importing necessary modules, set up some commonly used paths, and ingest our data. This data comes courtesy of Arab Academy on Roboflow:\n",
    "- https://universe.roboflow.com/arab-academy-vf9su/dental-7yegp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "import torchvision\n",
    "torchvision.disable_beta_transforms_warning()\n",
    "\n",
    "from torchvision import models, datasets\n",
    "from torchvision.datasets import (\n",
    "    CocoDetection,\n",
    "    wrap_dataset_for_transforms_v2\n",
    ")\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "from types import SimpleNamespace\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from typing import Optional, Tuple, Dict, Any\n",
    "from collections import defaultdict\n",
    "from tqdm.cli import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mask_rcnn_model'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msys\u001b[39;00m\n\u001b[32m      2\u001b[39m sys.path.append(\u001b[33m\"\u001b[39m\u001b[33m../src/\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmask_rcnn_model\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MaskRCNNTrainer\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvisualization_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      6\u001b[39m     plot_coco_image,\n\u001b[32m      7\u001b[39m     plot_coco_image_prediction,\n\u001b[32m      8\u001b[39m     plot_coco_image_predictions\n\u001b[32m      9\u001b[39m )\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmask_rcnn_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     11\u001b[39m     get_cocodetection_dataset,\n\u001b[32m     12\u001b[39m     custom_collate_function,\n\u001b[32m   (...)\u001b[39m\u001b[32m     15\u001b[39m     CATEGORY_ID_TO_NAME\n\u001b[32m     16\u001b[39m )\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'mask_rcnn_model'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src/\")\n",
    "\n",
    "from mask_rcnn_model import MaskRCNNTrainer\n",
    "from visualization_utils import (\n",
    "    plot_coco_image,\n",
    "    plot_coco_image_prediction,\n",
    "    plot_coco_image_predictions\n",
    ")\n",
    "from mask_rcnn_utils import (\n",
    "    get_cocodetection_dataset,\n",
    "    custom_collate_function,\n",
    "    process_output,\n",
    "    process_outputs,\n",
    "    CATEGORY_ID_TO_NAME\n",
    ")\n",
    "\n",
    "from metric_utils import (\n",
    "    mask_iou_matrix,\n",
    "    match_predicted_and_true_masks,\n",
    "    evaluate_instance_segmentation,\n",
    "    evaluate_instance_segmentation_multiple_thresholds\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FPATHS = SimpleNamespace()\n",
    "FPATHS.data = Path(\"../data/\")\n",
    "\n",
    "FPATHS.data_train = FPATHS.data / \"train\"\n",
    "FPATHS.data_valid = FPATHS.data / \"valid\"\n",
    "FPATHS.data_test = FPATHS.data / \"test\"\n",
    "\n",
    "FPATHS.data_annotations_fname = \"_annotations.coco.json\"\n",
    "FPATHS.data_train_annotations = FPATHS.data_train / FPATHS.data_annotations_fname\n",
    "FPATHS.data_valid_annotations = FPATHS.data_valid / FPATHS.data_annotations_fname\n",
    "FPATHS.data_test_annotations = FPATHS.data_test / FPATHS.data_annotations_fname\n",
    "\n",
    "FPATHS.models = Path(\"../models/mask_rcnn/\")\n",
    "FPATHS.logs = Path(\"../logs/mask_rcnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"maskrcnn_resnet50_fpn_v2\"\n",
    "MAX_EPOCHS = 50\n",
    "\n",
    "# Memory constraints limits us to only 2 observations/batch (8GB VRAM)\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "# (<inf For Testing purposes) - only run these number of batches in training dataset\n",
    "MAX_BATCHES_TRAIN = np.inf\n",
    "MAX_BATCHES_VALID = np.inf\n",
    "# MAX_BATCHES_TRAIN = 16\n",
    "# MAX_BATCHES_VALID = MAX_BATCHES_TRAIN // 3\n",
    "\n",
    "LEARNING_RATE = 1e-3\n",
    "EARLY_STOP_PATIENCE = 5\n",
    "\n",
    "# Learning rate schedule parameters (Reduce LR on Plateau)\n",
    "LR_SCHEDULER_FACTOR = 0.1\n",
    "LR_SCHEDULER_PATIENCE = 3\n",
    "\n",
    "# How many batches to run through before logging our metrics\n",
    "LOG_BATCH_INTERVAL = 128\n",
    "LOG_VALID_IMAGES = BATCH_SIZE\n",
    "\n",
    "assert LOG_VALID_IMAGES <= BATCH_SIZE and LOG_VALID_IMAGES >= 0, (\n",
    "    \"LOG_VALID_IMAGES must be non-negative and not larger than BATCH_SIZE\"\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"\"\"\n",
    "Model Training \n",
    "- Model:                   {MODEL_NAME}\n",
    "- Initial Learning Rate:   {LEARNING_RATE}\n",
    "- Max Epochs:              {MAX_EPOCHS}\n",
    "- Ealy Stop Patience:      {EARLY_STOP_PATIENCE}\n",
    "\n",
    "Data Loading\n",
    "- Batch Size:              {BATCH_SIZE}\n",
    "- Maximum Batches (Train): {MAX_BATCHES_TRAIN}\n",
    "- Maximum Batches (Valid): {MAX_BATCHES_VALID}\n",
    "\n",
    "Logging\n",
    "- Log Batch Interval:      {LOG_BATCH_INTERVAL} (Based on Effective Batch Size)\n",
    "- Log Validation Images:   {LOG_VALID_IMAGES} (# Of Validation Images to Display)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Augmentations and Train/Val/Test Datasets\n",
    "Since data provides bounding boxes, masks, and segmentations in the [COCO format](https://cocodataset.org/#home), we can utilize `torchvision.datasets.CocoDetection` class to help load the data in as a PyTorch Dataset.\n",
    "\n",
    "We also define the augmentations to integrate with our training observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = SimpleNamespace()\n",
    "datasets.train = get_cocodetection_dataset(FPATHS.data_train, FPATHS.data_train_annotations, True)\n",
    "datasets.valid = get_cocodetection_dataset(FPATHS.data_valid, FPATHS.data_valid_annotations, False)\n",
    "datasets.test = get_cocodetection_dataset(FPATHS.data_test, FPATHS.data_test_annotations, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Instances\n",
    "Let's build out some functionality to visualize the bounding boxes, masks, and segmentations of each observation. We define `plot_coco_image()` for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, target = datasets.train[1]\n",
    "coco_image_example = plot_coco_image(\n",
    "    image,\n",
    "    target,\n",
    "    plot_masks=True,\n",
    "    plot_bboxes=True,\n",
    "    plot_segmentations=True,\n",
    "    plot_category_id=True,\n",
    "    category_names=CATEGORY_ID_TO_NAME,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a PyTorch `DataLoader`\n",
    "Dataloader's help us retrieve a batch of training samples for more seamless model training training. Since PyTorch expects each batch to be constant, we have to update our `collate_fn`. We define `custom_collate_function()` to represent each batch as a tuple `(images, targets)`. To ensure we can run the network on our GPU, we also have to move the images and certain targets to the GPU.\n",
    "\n",
    "Here, we can the structure of each batch:\n",
    "\n",
    "```python\n",
    "# Start iterating through the dataloader\n",
    "images, targets = next(iter(dataloader))\n",
    "\n",
    "# Extracts the first instance in the batch\n",
    "image = images[0]\n",
    "target = targets[0]\n",
    "\n",
    "# Access the data in the target\n",
    "target_bboxes = target[\"bbox\"]\n",
    "target_areas = target[\"area\"]\n",
    "target_segmentation = target[\"segmentation\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = SimpleNamespace()\n",
    "dataloaders.train = DataLoader(\n",
    "    datasets.train,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=custom_collate_function\n",
    ")\n",
    "\n",
    "dataloaders.valid = DataLoader(\n",
    "    datasets.valid,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    collate_fn=custom_collate_function\n",
    ")\n",
    "\n",
    "dataloaders.test = DataLoader(\n",
    "    datasets.test,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    collate_fn=custom_collate_function\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model - Mask RCNN\n",
    "\n",
    "For instance segmentation, PyTorch provides [Mask R-CNN](https://pytorch.org/vision/0.9/models.html#object-detection-instance-segmentation-and-person-keypoint-detection).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to `True` if we want to train the model;\n",
    "# Otherwise skip and use previously-stored model checkpoints\n",
    "train_model = False\n",
    "\n",
    "if train_model:\n",
    "    mask_rcnn = models.get_model(\n",
    "        MODEL_NAME,\n",
    "        weights=models.detection.MaskRCNN_ResNet50_FPN_V2_Weights.COCO_V1,\n",
    "        weights_backbone=models.ResNet50_Weights.IMAGENET1K_V2,\n",
    "        trainable_backbone_layers=2\n",
    "    )\n",
    "    \n",
    "    mask_rcnn.cuda()\n",
    "    \n",
    "    mask_rcnn_trainer = MaskRCNNTrainer(\n",
    "        mask_rcnn,\n",
    "        dataloaders.train,\n",
    "        dataloaders.valid,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        scheduler_factor=LR_SCHEDULER_FACTOR,\n",
    "        scheduler_patience=LR_SCHEDULER_PATIENCE,\n",
    "        early_stop_patience=EARLY_STOP_PATIENCE,\n",
    "        log_batches_interval=LOG_BATCH_INTERVAL,\n",
    "        fpath_logs=FPATHS.logs,\n",
    "        fpath_models=FPATHS.models,\n",
    "        max_batches_train=MAX_BATCHES_TRAIN,\n",
    "        max_batches_valid=MAX_BATCHES_VALID\n",
    "    )\n",
    "    \n",
    "    mask_rcnn_trainer.train(MAX_EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Model Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the best model checkpoint (defined by the model with the lowest valid_loss)\n",
    "FPATHS.model_checkpoints = FPATHS.models.glob(\"*.pth\")\n",
    "checkpoint = min(\n",
    "    (torch.load(checkpoint) for checkpoint in FPATHS.model_checkpoints),\n",
    "    key=lambda chkpt: chkpt[\"valid_loss\"]\n",
    ")\n",
    "\n",
    "# Load the model state dict\n",
    "model = models.get_model(MODEL_NAME, weights=None, weights_backbone=None)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "model.cuda()\n",
    "model.eval()\n",
    "\n",
    "# Load the optimizer state dict - MaskRCNNTrainer uses Adam\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "\n",
    "# Load the scheduler state dict - MaskRCNNTrainer uses ReduceLROnPlateau\n",
    "scheduler = ReduceLROnPlateau(optimizer)\n",
    "scheduler.load_state_dict(checkpoint[\"scheduler_state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some predictions with the testing data\n",
    "images, targets = next(iter(dataloaders.test))\n",
    "with torch.no_grad():\n",
    "    outputs = model(images, targets)\n",
    "\n",
    "outputs = process_outputs(outputs, iou_threshold=0.6, score_threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_tensors_in_dict_to_cpu(d: Dict[Any, Any]) -> Dict[Any, Any]:\n",
    "    for key, val in d.items():\n",
    "        if isinstance(val, torch.Tensor):\n",
    "            d[key] = val.cpu()\n",
    "\n",
    "    return d\n",
    "\n",
    "image_array_fig = plot_coco_image_predictions(\n",
    "    images.cpu(),\n",
    "    outputs,\n",
    "    (move_tensors_in_dict_to_cpu(target) for target in targets),\n",
    "    category_names=CATEGORY_ID_TO_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate metrics at IoU threshold = 0.5\n",
    "iou_threshold = 0.5\n",
    "metrics = evaluate_instance_segmentation(\n",
    "    model,\n",
    "    dataloaders.test,\n",
    "    iou_threshold=iou_threshold\n",
    ")\n",
    "\n",
    "display(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is more rigorous to use multiple thresholds\n",
    "# COCO moves from 0.5 to 0.95 in 10 steps of 0.05\n",
    "iou_thresholds = np.arange(0.5, 1, step=0.05)\n",
    "metrics_multiple = evaluate_instance_segmentation_multiple_thresholds(\n",
    "    model,\n",
    "    dataloaders.test,\n",
    "    iou_thresholds\n",
    ")\n",
    "\n",
    "# Take a quick looks at our averaged metrics\n",
    "display(metrics_multiple[-1])\n",
    "\n",
    "# Check that calculations match @ 0.5\n",
    "display(metrics_multiple[np.argwhere(iou_thresholds == iou_threshold).item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
